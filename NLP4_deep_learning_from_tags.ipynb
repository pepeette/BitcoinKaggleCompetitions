{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP4 deep learning from tags",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZRSiDYNQLSr"
      },
      "source": [
        "# "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvTemFCOPqr4"
      },
      "source": [
        "applying deep learning on twitterâ€™s sentiment analysis\n",
        "\n",
        "*   Train Model - use keras to build and train a deep neural network model\n",
        "\n",
        "*   Evaluate Model - measure the accuracy of the predictive model, and suggest further improvements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83bZgjYjwUZk"
      },
      "source": [
        "IMPORTING DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87T5BetZQYQv"
      },
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import datetime\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGgPw6lXwgIK",
        "outputId": "f865a5f5-a0fb-46b8-9c84-bf913f6d4bbc"
      },
      "source": [
        "#being able to read csv stored in google drive \n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "fMujS02_wcVJ",
        "outputId": "43c9247c-47da-48c1-b650-226a3d2bb0f8"
      },
      "source": [
        "# Reading the dataset with no columns titles and with latin encoding \n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP/tweetsClean.csv')\n",
        "df.sample(3)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>clean</th>\n",
              "      <th>url</th>\n",
              "      <th>tags</th>\n",
              "      <th>promote</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>544671</th>\n",
              "      <td>544671</td>\n",
              "      <td>2018-07-05</td>\n",
              "      <td>2018</td>\n",
              "      <td>kobo btc usd ngn zar kes kobocoin</td>\n",
              "      <td></td>\n",
              "      <td>Kobocoin</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562714</th>\n",
              "      <td>562714</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>2018</td>\n",
              "      <td>every buy buy every sell buy remeber folks btc...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2795051</th>\n",
              "      <td>2795051</td>\n",
              "      <td>2019-07-01</td>\n",
              "      <td>2019</td>\n",
              "      <td>onecoin ceo denied bail read crypto jail nyc b...</td>\n",
              "      <td>https://t.co/Egi2CkzYp5</td>\n",
              "      <td>bitcoin jail nyc crypto</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Unnamed: 0        date  ...                     tags promote\n",
              "544671       544671  2018-07-05  ...                 Kobocoin        \n",
              "562714       562714  2018-07-31  ...                                 \n",
              "2795051     2795051  2019-07-01  ...  bitcoin jail nyc crypto        \n",
              "\n",
              "[3 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOckP_sYQcda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c6d04e-d51f-4961-904e-2477a044363d"
      },
      "source": [
        "# Checking if there is any missing value and datatype \n",
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7080772 entries, 0 to 7080771\n",
            "Data columns (total 7 columns):\n",
            " #   Column      Dtype \n",
            "---  ------      ----- \n",
            " 0   Unnamed: 0  int64 \n",
            " 1   date        object\n",
            " 2   year        int64 \n",
            " 3   clean       object\n",
            " 4   url         object\n",
            " 5   tags        object\n",
            " 6   promote     object\n",
            "dtypes: int64(2), object(5)\n",
            "memory usage: 378.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F9FfyFSQl_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c3b3f17-168d-48b0-aa5b-99d1af2143c5"
      },
      "source": [
        "\n",
        "# checking for null values, if any\n",
        "df.isnull().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0        0\n",
              "date              0\n",
              "year              0\n",
              "clean         15390\n",
              "url               0\n",
              "tags           9289\n",
              "promote           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xalsFDNyQp66"
      },
      "source": [
        "#ditching all row when text is null, as need text for analysis\n",
        "df.dropna(how='any', inplace=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XryjXIYhQ_gM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "b89b1226-f668-4a68-fa1c-bee1e76f45ec"
      },
      "source": [
        "df.sample(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>clean</th>\n",
              "      <th>url</th>\n",
              "      <th>tags</th>\n",
              "      <th>promote</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>797778</th>\n",
              "      <td>797778</td>\n",
              "      <td>2019-01-19</td>\n",
              "      <td>2019</td>\n",
              "      <td>exchange make fiat pairing available least for...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3485925</th>\n",
              "      <td>3485925</td>\n",
              "      <td>2019-07-22</td>\n",
              "      <td>2019</td>\n",
              "      <td>use referral link sign get usd crypto btc mco cro</td>\n",
              "      <td>https://t.co/S3qN709MjY https://t.co/qlc2UYkIzd</td>\n",
              "      <td>btc cro mco crypto</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3006068</th>\n",
              "      <td>3006068</td>\n",
              "      <td>2019-07-08</td>\n",
              "      <td>2019</td>\n",
              "      <td>binance btc market elf unusual selling activit...</td>\n",
              "      <td></td>\n",
              "      <td>ELF</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Unnamed: 0        date  ...                tags promote\n",
              "797778       797778  2019-01-19  ...                            \n",
              "3485925     3485925  2019-07-22  ...  btc cro mco crypto        \n",
              "3006068     3006068  2019-07-08  ...                 ELF        \n",
              "\n",
              "[3 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1NKt61m4r9H"
      },
      "source": [
        "stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcwO8vMB4vNr"
      },
      "source": [
        "testing some embedding for deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCtmOhQ840y-"
      },
      "source": [
        "embed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRa095DbE6J0",
        "outputId": "38919a21-a438-41a1-a276-2cde1639f95b"
      },
      "source": [
        "df['tags'].sample(10000)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('O')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwnF6Dsd42Ek"
      },
      "source": [
        "# Defining the window for context\r\n",
        "window = 2\r\n",
        "\r\n",
        "# Creating a placeholder for the scanning of the word list\r\n",
        "word_lists = []\r\n",
        "all_text = []\r\n",
        "\r\n",
        "for text in df['tags'].sample(10000):\r\n",
        "\r\n",
        "    # Appending to the all text list\r\n",
        "    all_text += text \r\n",
        "\r\n",
        "    # Creating a context dictionary\r\n",
        "    for i, word in enumerate(text):\r\n",
        "        for w in range(window):\r\n",
        "            # Getting the context that is ahead by *window* words\r\n",
        "            if i + 1 + w < len(text): \r\n",
        "                word_lists.append([word] + [text[(i + 1 + w)]])\r\n",
        "            # Getting the context that is behind by *window* words    \r\n",
        "            if i - w - 1 >= 0:\r\n",
        "                word_lists.append([word] + [text[(i - w - 1)]])\r\n",
        "\r\n",
        "print(len(word_lists)), print(len(all_text))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtU19Kih81op"
      },
      "source": [
        "def create_unique_word_dict(text:list) -> dict:\r\n",
        "    \"\"\"\r\n",
        "    A method that creates a dictionary where the keys are unique words\r\n",
        "    and key values are indices\r\n",
        "    \"\"\"\r\n",
        "    # Getting all the unique words from our text and sorting them alphabetically\r\n",
        "    words = list(set(text))\r\n",
        "    words.sort()\r\n",
        "\r\n",
        "    # Creating the dictionary for the unique words\r\n",
        "    unique_word_dict = {}\r\n",
        "    for i, word in enumerate(words):\r\n",
        "        unique_word_dict.update({\r\n",
        "            word: i\r\n",
        "        })\r\n",
        "\r\n",
        "    return unique_word_dict "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGa1L3Ca85nS",
        "outputId": "f8cda028-89ab-4753-f2b6-e050980911f1"
      },
      "source": [
        "unique_word_dict = create_unique_word_dict(all_text)\r\n",
        "unique_word_dict"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 0, 'c': 1, 'e': 2, 'l': 3, 'n': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKyr7Ld-5qvL"
      },
      "source": [
        "# Defining the number of features (unique words)\r\n",
        "n_words = len(unique_word_dict)\r\n",
        "\r\n",
        "# Getting all the unique words \r\n",
        "words = list(unique_word_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD8GTqmL5vwv"
      },
      "source": [
        "# Creating the X and Y matrices using one hot encoding\r\n",
        "X = []\r\n",
        "Y = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb4SCgab6HXU"
      },
      "source": [
        "for i, word_list in tqdm(enumerate(word_lists)):\r\n",
        "    # Getting the indices\r\n",
        "    main_word_index = unique_word_dict.get(word_list[0])\r\n",
        "    context_word_index = unique_word_dict.get(word_list[1])\r\n",
        "\r\n",
        "    # Creating the placeholders   \r\n",
        "    X_row = np.zeros(n_words)\r\n",
        "    Y_row = np.zeros(n_words)\r\n",
        "\r\n",
        "    # One hot encoding the main word\r\n",
        "    X_row[main_word_index] = 1\r\n",
        "\r\n",
        "    # One hot encoding the Y matrix words \r\n",
        "    Y_row[context_word_index] = 1\r\n",
        "\r\n",
        "    # Appending to the main matrices\r\n",
        "    X.append(X_row)\r\n",
        "    Y.append(Y_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNw-sRgK6Nla"
      },
      "source": [
        "# Converting the matrices into a sparse format because the vast majority of the data are 0s\r\n",
        "X = sparse.csr_matrix(X)\r\n",
        "Y = sparse.csr_matrix(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKQi8Gx7_CyJ"
      },
      "source": [
        "We now have X and Y matrices built from the focus word and context word pairs. The next step is to choose the embedding dimension. I will choose the dimension to be equal to 2 in order to later plot the words and see whether similar words form clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXwOoEk77OkT"
      },
      "source": [
        "# Defining the size of the embedding\r\n",
        "embed_size = 2\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESPFNWW__lD1"
      },
      "source": [
        "The output layers activation function is softmax. The activation function of the hidden layer is linear. The input dimension is equal to the total number of unique words (remember, our X matrix is of the dimension n x 21). Each input node will have two weights connecting it to the hidden layer. These weights are the word embeddings! After the training of the network, we extract these weights and remove all the rest. We do not necessarily care about the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02Uy74WH7S6t"
      },
      "source": [
        "# Defining the neural network\r\n",
        "inp = Input(shape=(X.shape[1],))\r\n",
        "x = Dense(units=embed_size, activation='linear')(inp)\r\n",
        "x = Dense(units=Y.shape[1], activation='softmax')(x)\r\n",
        "model = Model(inputs=inp, outputs=x)\r\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIzIbjJs7W_U"
      },
      "source": [
        "# Optimizing the network weights\r\n",
        "model.fit(\r\n",
        "    x=X, \r\n",
        "    y=Y, \r\n",
        "    batch_size=256,\r\n",
        "    epochs=1000\r\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDoQeKgE7f09"
      },
      "source": [
        "# Obtaining the weights from the neural network. \r\n",
        "# These are the so called word embeddings\r\n",
        "\r\n",
        "# The input layer \r\n",
        "weights = model.get_weights()[0]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TyRersM7g2Y"
      },
      "source": [
        "# Creating a dictionary to store the embeddings in. The key is a unique word and \r\n",
        "# the value is the numeric vector\r\n",
        "embedding_dict = {}\r\n",
        "for word in words: \r\n",
        "    embedding_dict.update({\r\n",
        "        word: weights[unique_word_dict.get(word)]\r\n",
        "        })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZNvc5Gr7n_X"
      },
      "source": [
        "# Ploting the embeddings\r\n",
        "plt.figure(figsize=(10, 10))\r\n",
        "for word in list(unique_word_dict.keys()):\r\n",
        "    coord = embedding_dict.get(word)\r\n",
        "    plt.scatter(coord[0], coord[1])\r\n",
        "    plt.annotate(word, (coord[0], coord[1]))       \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnC3Thoi8zn5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBjUmI-Gtt_H"
      },
      "source": [
        "EXTRACTING FEATURES FROM CLEANED TWEETS 10 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNJz_AVvtyqu"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ1nCxQJts15",
        "outputId": "ff3077b8-7838-4c37-b186-8a3ee072b91b"
      },
      "source": [
        "#bag of words = OPTION A\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "bow = bow_vectorizer.fit_transform(df['clean'])\n",
        "bow.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7056094, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ6aZevKt4T0",
        "outputId": "62603802-695c-47e7-f1e4-9f8aec4d5ac4"
      },
      "source": [
        "#TfIdf = OPTION B \n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(df['clean'])\n",
        "tfidf.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7056094, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA26z4laQyYm"
      },
      "source": [
        "Word 2 Vec : KeyError: \"word 'eth vs btc relative vol spread interesting junction esp given btc dominance v alt season sentiment participants cryptooptions releativevalue' not in vocabulary\"\n",
        "or  \"word 'bizpaye trading platform system unique never done history modern day trade exchanges bizpaye marketplace hodl bartercredit crypto cryptotrading btc onlineshopping merchants ecommerce bb bc retail' not in vocabulary\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu-jU0iUk0T0"
      },
      "source": [
        "PREPARE FOR MODELING\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCpmVRAQ6Z1u"
      },
      "source": [
        "DEFINING X and Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6H3n3zEnMVB"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# ML Libraries\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X57bQvLlLdM"
      },
      "source": [
        "#1- vectoring data\n",
        "def get_feature_vector(train_fit):\n",
        "    vector = TfidfVectorizer(sublinear_tf=True)\n",
        "    vector.fit(train_fit)\n",
        "    return vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "jQTMk4whbMEH",
        "outputId": "f332af9d-d4ec-4bae-f0be-bd4ad4ae600a"
      },
      "source": [
        "#2- CREATING a FAKE Y\n",
        "#ate 11 dec 2017\n",
        "#ate 10 dec 2018\n",
        "#ate end\n",
        "\n",
        "def senti(x):\n",
        "  if x < 2018:\n",
        "    return 'BULL'\n",
        "  elif x > 2018:\n",
        "    return 'BULL2'\n",
        "  else:\n",
        "    return 'BEAR'\n",
        "\n",
        "df['sent'] = df['year'].apply(lambda x: senti(x) )\n",
        "df.tail(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>text1</th>\n",
              "      <th>url</th>\n",
              "      <th>tags</th>\n",
              "      <th>promote</th>\n",
              "      <th>clean</th>\n",
              "      <th>sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7080769</th>\n",
              "      <td>21513683</td>\n",
              "      <td>2019-11-23</td>\n",
              "      <td>@ABC Setup your FREE account Now : https://t.c...</td>\n",
              "      <td>2019</td>\n",
              "      <td>11</td>\n",
              "      <td>23</td>\n",
              "      <td>@ABC Setup your FREE account Now : https://t.c...</td>\n",
              "      <td>https://t.co/J2f8AlXFqZ https://t.co/J2f8AlXFqZ</td>\n",
              "      <td>Crypto Bitcoin btc Cryptocurrency BTC</td>\n",
              "      <td></td>\n",
              "      <td>setup free account automatic bitcome get paid ...</td>\n",
              "      <td>BULL2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7080770</th>\n",
              "      <td>21513685</td>\n",
              "      <td>2019-11-23</td>\n",
              "      <td>@OJRenick So you don't need bitcoin, aye? http...</td>\n",
              "      <td>2019</td>\n",
              "      <td>11</td>\n",
              "      <td>23</td>\n",
              "      <td>@OJRenick So you don't need bitcoin, aye? http...</td>\n",
              "      <td>https://t.co/F8QCKgKM8Y</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>need bitcoin aye</td>\n",
              "      <td>BULL2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7080771</th>\n",
              "      <td>21540059</td>\n",
              "      <td>2019-11-23</td>\n",
              "      <td>$BTC - an update on the longer term view for B...</td>\n",
              "      <td>2019</td>\n",
              "      <td>11</td>\n",
              "      <td>23</td>\n",
              "      <td>$BTC - an update on the longer term view for B...</td>\n",
              "      <td>https://t.co/yBEMdy9pwp</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>btc update longer term view btc price action s...</td>\n",
              "      <td>BULL2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Unnamed: 0  ...   sent\n",
              "7080769    21513683  ...  BULL2\n",
              "7080770    21513685  ...  BULL2\n",
              "7080771    21540059  ...  BULL2\n",
              "\n",
              "[3 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAVHqoGPn36j",
        "outputId": "b4a4ca7c-6bc1-477d-8c21-734daea759ca"
      },
      "source": [
        "df['sent'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BULL2    6310080\n",
              "BEAR      507078\n",
              "BULL      238936\n",
              "Name: sent, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GVBlrYOmRME"
      },
      "source": [
        "#splitting - takes 5 minutes\n",
        "tf_vector = get_feature_vector(np.array(df['clean']).ravel())\n",
        "X = tf_vector.transform(np.array(df['clean']).ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE75AKBckPOL",
        "outputId": "bb25c0fd-176f-4b13-ce37-c331145ac127"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x950508 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 6 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnPHr_h3sOFU"
      },
      "source": [
        "y = np.array(df['sent']).ravel()\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq5eSFg4PURa"
      },
      "source": [
        "LAUNCHING MODEL BASES after 1 hour of running the preproc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UplF8soinXXS",
        "outputId": "2f2f9c44-3779-4be6-98a8-5029ea18013a"
      },
      "source": [
        "# Training Naive Bayes model\n",
        "NB_model = MultinomialNB()\n",
        "NB_model.fit(X_train, y_train)\n",
        "y_predict_nb = NB_model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_predict_nb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9115918196509969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFggilGzndGa",
        "outputId": "86a44d7c-b32d-4238-b742-4cdb49244843"
      },
      "source": [
        "# Training Logistics Regression model - reducing to solver lbfgs for 5 min cause libelinear or newton_cg are to expansive and take 12 good minutes\n",
        "LR_model = LogisticRegression(solver='lbfgs', max_iter=100)\n",
        "LR_model.fit(X_train, y_train)\n",
        "y_predict_lr = LR_model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_predict_lr))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9313789635346077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZTm8_TPP4gF"
      },
      "source": [
        "#SVM - takes 15 min\n",
        "from sklearn import svm\n",
        "svc = svm.SVC(kernel='linear')\n",
        "svc.fit(X_train, y_train)\n",
        "y_predict_svm =  svc.predict_proba(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slYEjO-4g6mn"
      },
      "source": [
        "from sklearn import metrics\r\n",
        "\r\n",
        "# Model Accuracy: how often is the classifier correct?\r\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predict_svm))\r\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\r\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_predict_svm))\r\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\r\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_predict_svm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkdGcJn6RJZq"
      },
      "source": [
        "#Training Random Forest still nothing after 37 mn\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
        "rf.fit(X_train, y_train) \n",
        "y_predict_rf = rf.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRorSS7DS9P3"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(accuracy_score(y_test, y_predict_rf))\n",
        "print(confusion_matrix(y_test,y_predict_rf))\n",
        "print(classification_report(y_test,y_predict_rf))\n",
        "print(accuracy_score(y_test, y_predict_rf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMl_nu3-Rjfy"
      },
      "source": [
        "# TRaining XGB\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(X_train, y_train)\n",
        "y_predict_xgb = xgb.predict(X_test)\n",
        "print(accuracy_score(yvalid, y_predict_xgb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wBlgh4KRn9j"
      },
      "source": [
        "GO FURTHER\n",
        "score pour chaque column / mot - lesquels ont ete le plus utilise pour predire\n",
        "carac du model ou PCA (mix de col qui marchent le mieux, qu est ce qui max la variance et apporte le plus d info) => qu est ce qui a ete utilise le plus par le modele"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXuGlHvCFjKo"
      },
      "source": [
        "ajouter d autres colonnes avec  ou essayer d autres modeles comme RF ou classifier plus finement les Y ou faire un clustering non supervise, si pas de Y, patterns par time, plusieurs clusters, can it work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOmywvEm8WYs"
      },
      "source": [
        "mport numpy as np\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "     \r\n",
        "\r\n",
        "class TextToTensor():\r\n",
        "\r\n",
        "    def __init__(self, tokenizer, max_len):\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.max_len = max_len\r\n",
        "\r\n",
        "    def string_to_tensor(self, string_list: list) -> list:\r\n",
        "        \"\"\"\r\n",
        "        A method to convert a string list to a tensor for a deep learning model\r\n",
        "        \"\"\"    \r\n",
        "        string_list = self.tokenizer.texts_to_sequences(string_list)\r\n",
        "        string_list = pad_sequences(string_list, maxlen=self.max_len)\r\n",
        "        \r\n",
        "        return string_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YWmSpmx8iVW"
      },
      "source": [
        "# Tokenizing the text\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "# Getting the longest sentence\r\n",
        "max_len = np.max([len(text.split()) for text in X_train])\r\n",
        "# Converting to tensor\r\n",
        "TextToTensor_instance = TextToTensor(\r\n",
        "tokenizer=tokenizer,\r\n",
        "max_len=max_len\r\n",
        ")\r\n",
        "X_train_NN = TextToTensor_instance.string_to_tensor(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3SIsqv18ksF"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Embedding\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(\r\n",
        "  input_dim=44, \r\n",
        "  output_dim=3, \r\n",
        "  input_length=max_len))\r\n",
        "\r\n",
        "model.compile('rmsprop', 'mse')\r\n",
        "output_array = model.predict(X_train_NN)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Rn98ld8oWV"
      },
      "source": [
        "embed_path = 'embeddings\\\\glove.840B.300d.txt'\r\n",
        "embed_dim = 300\r\n",
        "# Tokenizing the text\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "# Creating the embedding matrix\r\n",
        "embedding = Embeddings(embed_path, embed_dim)\r\n",
        "embedding_matrix = embedding.create_embedding_matrix(tokenizer, len(tokenizer.word_counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1_bBfisL0D8"
      },
      "source": [
        "FINE TUNING\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJqa02aRSFE5"
      },
      "source": [
        "Feature importance :\r\n",
        "\r\n",
        "*   from model coefficients.\r\n",
        "*   from decision trees\r\n",
        "*   from permutation testing.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2sX_EpnSGJS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}